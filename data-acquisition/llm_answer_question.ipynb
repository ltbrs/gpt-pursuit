{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lambert-bruyas/external_git/gpt-pursuit/data-acquisition/src/data.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df[\"Category\"] = datasets[sheet_name][\"Category\"+suffix]\n",
      "/home/lambert-bruyas/external_git/gpt-pursuit/data-acquisition/src/data.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df[\"Category\"] = datasets[sheet_name][\"Category\"+suffix]\n",
      "/home/lambert-bruyas/external_git/gpt-pursuit/data-acquisition/src/data.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df[\"Category\"] = datasets[sheet_name][\"Category\"+suffix]\n"
     ]
    }
   ],
   "source": [
    "from src.data import get_questions_df\n",
    "\n",
    "question_df = get_questions_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. LLM challenger Answer a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lambert-bruyas/miniconda3/envs/gpt_pursuit/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.consts import SELECTED_LLMS\n",
    "from src.answer import llms_answer_questions\n",
    "from src.orchestrator import (\n",
    "    get_missing_question_ids, \n",
    "    load_answered_questions, BATCH_SIZE,\n",
    "    save_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "answered_questions = load_answered_questions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_ids = get_missing_question_ids(\n",
    "    answered_questions,\n",
    "    question_df[\"id\"].tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_sample = question_df.set_index('id').loc[missing_ids].sample(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lambert-bruyas/external_git/gpt-pursuit/data-acquisition/models/HuggingFaceTB/SmolLM2-360M-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['local_files_only'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m question_answers \u001b[38;5;241m=\u001b[39m \u001b[43mllms_answer_questions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion_sample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mQuestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSELECTED_LLMS\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/external_git/gpt-pursuit/data-acquisition/src/answer.py:16\u001b[0m, in \u001b[0;36mllms_answer_questions\u001b[0;34m(questions, llms)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mllms_answer_questions\u001b[39m(questions: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m], llms: \u001b[38;5;28mlist\u001b[39m[LLMConfig]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]]:\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43manswer_questions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mllms\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/external_git/gpt-pursuit/data-acquisition/src/answer.py:16\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mllms_answer_questions\u001b[39m(questions: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m], llms: \u001b[38;5;28mlist\u001b[39m[LLMConfig]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]]:\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43manswer_questions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m llm \u001b[38;5;129;01min\u001b[39;00m llms]\n",
      "File \u001b[0;32m~/external_git/gpt-pursuit/data-acquisition/src/answer.py:11\u001b[0m, in \u001b[0;36manswer_questions\u001b[0;34m(questions, llm_config)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21manswer_questions\u001b[39m(questions: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m], llm_config: LLMConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m     10\u001b[0m     pipe \u001b[38;5;241m=\u001b[39m load_pipeline(llm_config)\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mllm_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbuild_pipeline_input\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEFAULT_BUILD_QUESTION\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPROMPT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/external_git/gpt-pursuit/data-acquisition/src/answer.py:11\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21manswer_questions\u001b[39m(questions: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m], llm_config: LLMConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m     10\u001b[0m     pipe \u001b[38;5;241m=\u001b[39m load_pipeline(llm_config)\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mllm_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbuild_pipeline_input\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEFAULT_BUILD_QUESTION\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPROMPT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m questions]\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt_pursuit/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:295\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first_item, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mdict\u001b[39m)):\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;66;03m# We have one or more prompts in list-of-dicts format, so this is chat mode\u001b[39;00m\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first_item, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 295\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mChat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    297\u001b[0m         chats \u001b[38;5;241m=\u001b[39m (Chat(chat) \u001b[38;5;28;01mfor\u001b[39;00m chat \u001b[38;5;129;01min\u001b[39;00m text_inputs)  \u001b[38;5;66;03m# ðŸˆ ðŸˆ ðŸˆ\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt_pursuit/lib/python3.11/site-packages/transformers/pipelines/base.py:1431\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1424\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1425\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1428\u001b[0m         )\n\u001b[1;32m   1429\u001b[0m     )\n\u001b[1;32m   1430\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt_pursuit/lib/python3.11/site-packages/transformers/pipelines/base.py:1438\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1436\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1437\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1438\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1439\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt_pursuit/lib/python3.11/site-packages/transformers/pipelines/base.py:1338\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1336\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1337\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1338\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1339\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt_pursuit/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:400\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[1;32m    398\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[0;32m--> 400\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[1;32m    403\u001b[0m     generated_sequence \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msequences\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt_pursuit/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt_pursuit/lib/python3.11/site-packages/transformers/generation/utils.py:2357\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2352\u001b[0m assistant_tokenizer \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# only used for assisted generation\u001b[39;00m\n\u001b[1;32m   2354\u001b[0m generation_config, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_generation_config(\n\u001b[1;32m   2355\u001b[0m     generation_config, use_model_defaults, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   2356\u001b[0m )\n\u001b[0;32m-> 2357\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_model_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2358\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_assistant(assistant_model, tokenizer, assistant_tokenizer)\n\u001b[1;32m   2360\u001b[0m \u001b[38;5;66;03m# 2. Set generation parameters if not already defined\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt_pursuit/lib/python3.11/site-packages/transformers/generation/utils.py:1599\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_kwargs\u001b[0;34m(self, model_kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m         unused_model_args\u001b[38;5;241m.\u001b[39mappend(key)\n\u001b[1;32m   1598\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unused_model_args:\n\u001b[0;32m-> 1599\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1600\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_model_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (note: typos in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1601\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m generate arguments will also show up in this list)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1602\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['local_files_only'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "source": [
    "question_answers = llms_answer_questions(question_sample[\"Question\"].tolist(), SELECTED_LLMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The recording engineer on \"Sgt. Pepper's Lonely Hearts Club Band\" was John Lennon. HuggingFaceTB/SmolLM2-360M-Instruct\n",
      "Apples, pears, and other fruits were commonly stored in pub cellars during the 17th century. HuggingFaceTB/SmolLM2-360M-Instruct\n",
      "The answer depends on the specific insect species being referred to. However, if we're talking about Hymenoptera, which includes ants, bees, wasps, and butterflies, the insect with the prominent pair of pincers at the tip of its abdomen is the female praying mantis. Pheromone spray is also a characteristic pheromone used by male praying mantises to attract females. HuggingFaceTB/SmolLM2-360M-Instruct\n",
      "South America is closer to the Antarctic. HuggingFaceTB/SmolLM2-360M-Instruct\n",
      "A leporine cat is a feline cat that belongs to the family of small, typically nocturnal mammals known as the LEPOROINS. The most common examples of leporine cats include the Leopard Longhair, Persian, and Cornish Rex. HuggingFaceTB/SmolLM2-360M-Instruct\n",
      "Harry Potter is a classic British comic book series based on the novel series of the same name by J.K. Rowling. The band was a group of musicians that played the songs of The Beatles' later albums \"Sgt. Pepper's Lonely Hearts Club Band\" and \"Masters of the Universe.\" They were a group of musicians who were not primarily based on the band's official name, but rather on the name of their first album, \"Sgt. Pepper's Lonely Hearts Club Band.\" HuggingFaceTB/SmolLM2-135M-Instruct\n",
      "I'm sorry for the misunderstanding, but as a quiz AI, I don't have the capability to perform trivia questions or interact with human users. However, I can suggest you do a search for \"purchasing in the pub cellars\" to see which pubs and bars have remained the most popular during the pandemic. HuggingFaceTB/SmolLM2-135M-Instruct\n",
      "Answers:\n",
      "\n",
      "1. \"Ectoparasitica\"\n",
      "\n",
      "2. \"Caterpillar fly\"\n",
      "\n",
      "3. \"Ladybird bug\" HuggingFaceTB/SmolLM2-135M-Instruct\n",
      "South America is closer to the Antarctic than Australia is. HuggingFaceTB/SmolLM2-135M-Instruct\n",
      "A cat is feline, but what's leporine? HuggingFaceTB/SmolLM2-135M-Instruct\n",
      "robert wilson google/flan-t5-small\n",
      "beer google/flan-t5-small\n",
      "a cockroach google/flan-t5-small\n",
      "South America google/flan-t5-small\n",
      "leporine google/flan-t5-small\n",
      "james mccartney google/flan-t5-base\n",
      "food google/flan-t5-base\n",
      "arachnids google/flan-t5-base\n",
      "south america or australia google/flan-t5-base\n",
      "leporine is a genus of flowering plants in the sunflower family google/flan-t5-base\n",
      "The recording engineer for \"Sgt. Pepper's Lonely Hearts Club Band\" was **Geoff Emerick**. \n",
      " google/gemma-2-2b-it\n",
      "Apart from drinks, pub cellars used to store **everything**!  Here are some of the common items:\n",
      "\n",
      "* **Firewood:**  To keep fires going for warmth.\n",
      "* **Tools and equipment:**  For repairs, cleaning, and general upkeep.\n",
      "* **Food:**  Preserved meats, cheeses, and other provisions.\n",
      "* **Wool and bedding:**  For warmth and comfort.\n",
      "* **Supplies:**  Candles, lanterns, and other lighting.\n",
      "* **Musical instruments:**  For entertainment.\n",
      "* **Gambling equipment:**  For games like cards, dice, and dominoes.\n",
      "* **Other valuables:**  Pottery, pewter, and other household items.\n",
      "\n",
      "Pub cellars were often multi-purpose spaces with a variety of different uses. \n",
      " google/gemma-2-2b-it\n",
      "The insect with a pair of prominent pincers at the tip of its abdomen is a **praying mantis**. \n",
      "\n",
      "Let me know if you'd like to tackle another trivia question! ðŸ¦— \n",
      " google/gemma-2-2b-it\n",
      "**Australia** is closer to the Antarctic. \n",
      "\n",
      "Here's why:\n",
      "\n",
      "* **Geographic Location:** Australia is located on the opposite side of the Indian Ocean from Antarctica.\n",
      "* **Distance:** The distance between Australia and Antarctica is roughly 2,000 to 4,000 kilometers (depending on the specific Australian location). This is significantly shorter than the distance between South America and Antarctica. \n",
      "\n",
      "\n",
      "Let me know if you'd like to try another trivia question! \n",
      " google/gemma-2-2b-it\n",
      "Leporine refers to the **hare family**, which is part of the order Lagomorpha. \n",
      "\n",
      "* **Feline** refers to a cat, belonging to the family Felidae. \n",
      " google/gemma-2-2b-it\n",
      "The recording engineer for The Beatles' album \"Sgt. Pepper's Lonely Hearts Club Band\" was Geoff Emerick. Qwen/Qwen2.5-1.5B-Instruct\n",
      "Apart from drinks, pubs also traditionally kept beer barrels and casks in their cellars for storage and preservation before they were served to customers. Qwen/Qwen2.5-1.5B-Instruct\n",
      "The insect you're referring to is likely an ant. Ants have two prominent pincers at the tip of their abdomen called mandibles which they use for defense and food acquisition. Qwen/Qwen2.5-1.5B-Instruct\n",
      "Australia is closer to the Antarctic than South America. This can be determined by comparing their geographical positions and distances from Antarctica.\n",
      "1. **Distance from Antarctica**: \n",
      "   - The distance between Antarctica and Australia ranges from about 4,000 km (2,500 miles) at its closest point to around 6,000 km (3,700 miles) at its furthest point.\n",
      "   - The average distance for both countries to reach Antarctica via the Southern Ocean is approximately 4,800 km (2,983 miles).\n",
      "\n",
      "2. **Geographical Position**:\n",
      "   - Australia's southernmost territory is Tasmania, which lies near the Antarctic Circle.\n",
      "   - South Americaâ€™s southernmost territories include Tierra del Fuego and parts of Chile and Argentina.\n",
      "\n",
      "Given these factors, it's clear that while both continents are geographically distant from each other, Australia is closer to the Antarctic due to its proximity to the continent itself rather than being further away through water routes like those used by South America. Qwen/Qwen2.5-1.5B-Instruct\n",
      "A leporine animal would be a rabbit. Rabbits belong to the family Leporidae and are closely related to cats as both are mammals from the order Lagomorpha. Qwen/Qwen2.5-1.5B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9886/1240836090.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  question_sample.loc[:, llm.pipeline_kwargs[\"model\"]] = model_answers\n",
      "/tmp/ipykernel_9886/1240836090.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  question_sample.loc[:, llm.pipeline_kwargs[\"model\"]] = model_answers\n",
      "/tmp/ipykernel_9886/1240836090.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  question_sample.loc[:, llm.pipeline_kwargs[\"model\"]] = model_answers\n",
      "/tmp/ipykernel_9886/1240836090.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  question_sample.loc[:, llm.pipeline_kwargs[\"model\"]] = model_answers\n",
      "/tmp/ipykernel_9886/1240836090.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  question_sample.loc[:, llm.pipeline_kwargs[\"model\"]] = model_answers\n",
      "/tmp/ipykernel_9886/1240836090.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  question_sample.loc[:, llm.pipeline_kwargs[\"model\"]] = model_answers\n"
     ]
    }
   ],
   "source": [
    "for i, llm in enumerate(SELECTED_LLMS):\n",
    "    model_answers:list[str] = []\n",
    "    for j, question in enumerate(question_answers[i]):\n",
    "        answer = question_answers[i][j][0]['generated_text']\n",
    "        if isinstance(answer, list):\n",
    "            answer = answer[-1]['content']\n",
    "        else:\n",
    "            pass\n",
    "        model_answers.append(answer)\n",
    "        print(answer, llm.pipeline_kwargs[\"model\"])\n",
    "    question_sample.loc[:, llm.pipeline_kwargs[\"model\"]] = model_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "from typing import TypedDict, NotRequired\n",
    "import pandas as pd\n",
    "from types import Question\n",
    "from datetime import datetime\n",
    "\n",
    "question_output:list[Question] = []\n",
    "\n",
    "for i in range(len(question_sample)):\n",
    "    question_output.append({\n",
    "        \"id\": int(question_sample.iloc[i][\"id\"]),\n",
    "        \"category\": question_sample.iloc[i][\"Category\"],\n",
    "        \"question\": question_sample.iloc[i][\"Question\"],\n",
    "        \"expected_answer\": question_sample.iloc[i][\"Answer\"],\n",
    "        \"llm_answers\": {llm.pipeline_kwargs[\"model\"]: {\n",
    "            \"answer\": question_sample.iloc[i].loc[llm.pipeline_kwargs[\"model\"]], \n",
    "            \"is_correct\": difflib.SequenceMatcher(None, question_sample.iloc[i][\"Answer\"], question_sample.iloc[i].loc[llm.pipeline_kwargs[\"model\"]]).ratio() > 0.8} for llm in SELECTED_LLMS},\n",
    "        \"difficulty\": question_sample.iloc[i][\"Difficulty\"] if not pd.isna(question_sample.iloc[i][\"Difficulty\"]) else None\n",
    "    })\n",
    "save_answers(question_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_pursuit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
