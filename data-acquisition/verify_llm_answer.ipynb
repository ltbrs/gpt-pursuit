{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import TypedDict\n",
    "\n",
    "\n",
    "class LLMAnswer(TypedDict):\n",
    "    answer: str\n",
    "    is_correct: bool\n",
    "\n",
    "class Question(TypedDict):\n",
    "    id: str\n",
    "    question: str\n",
    "    expected_answer: str\n",
    "    category: str\n",
    "    difficulty: str\n",
    "    llm_answers: dict[str, LLMAnswer]\n",
    "\n",
    "\n",
    "questions:list[Question] = json.load(open(\"../frontend/data/sample_trivia.json\"))['questions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test with your example\n",
    "test_case = {\n",
    "    \"question\": \"Who has built the Chateau de Versailles?\",\n",
    "    \"expected_answer\": \"Louis XIV\",\n",
    "    \"user_answer\": \"Louis XVI\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Xenova/distilbert-base-uncased-finetuned-sst2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lambert-bruyas/miniconda3/envs/gpt_pursuit/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'NEGATIVE', 'score': 0.9953383207321167}\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# Function to format input and classify\n",
    "def check_answer(question:str, expected_answer:str, user_answer:str):\n",
    "    input_text = f\"Question: {question} Expected answer: {expected_answer}. User answer: {user_answer}. Is the user answer correct? \"\n",
    "    result = classifier(input_text)\n",
    "    return result[0]\n",
    "\n",
    "\n",
    "\n",
    "result = check_answer(\n",
    "    test_case[\"question\"],\n",
    "    test_case[\"expected_answer\"],\n",
    "    test_case[\"user_answer\"]\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Approach: Containment Check + Semantic Similarity (all-mpnet-base-v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load a better sentence embedding model (all-mpnet-base-v2 has better accuracy)\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"Normalize text for comparison: lowercase, remove extra whitespace.\"\"\"\n",
    "    return re.sub(r'\\s+', ' ', text.lower().strip())\n",
    "\n",
    "def check_containment(expected_answer: str, user_answer: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if expected answer is contained in user answer.\n",
    "    Handles word boundaries for better matching.\n",
    "    \"\"\"\n",
    "    expected_norm = normalize_text(expected_answer)\n",
    "    user_norm = normalize_text(user_answer)\n",
    "    \n",
    "    # Exact match\n",
    "    if expected_norm == user_norm:\n",
    "        return True\n",
    "    \n",
    "    # Check if expected answer is contained as a substring\n",
    "    if expected_norm in user_norm:\n",
    "        return True\n",
    "    \n",
    "    # For short answers (1-3 words), check if all words are present\n",
    "    expected_words = set(expected_norm.split())\n",
    "    user_words = set(user_norm.split())\n",
    "    \n",
    "    # If expected answer is short (<= 3 words), check if all words appear\n",
    "    if len(expected_words) <= 3 and len(expected_words) > 0:\n",
    "        # All expected words must be in user answer\n",
    "        if expected_words.issubset(user_words):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def check_answer(question: str, expected_answer: str, user_answer: str, \n",
    "                 containment_threshold: int = 3, similarity_threshold: float = 0.55) -> bool:\n",
    "    \"\"\"\n",
    "    Hybrid approach: First check containment, then fall back to semantic similarity.\n",
    "    \n",
    "    Strategy:\n",
    "    1. If expected answer is contained in user answer (with word boundary handling), return True\n",
    "    2. Otherwise, use semantic similarity with question context\n",
    "    \n",
    "    Args:\n",
    "        question: The quiz question\n",
    "        expected_answer: The correct answer\n",
    "        user_answer: The user/LLM answer to validate\n",
    "        containment_threshold: Max words in expected answer to use containment check (default 3)\n",
    "        similarity_threshold: Semantic similarity threshold (default 0.55, lower for better recall)\n",
    "    \n",
    "    Returns:\n",
    "        True if answer is correct, False otherwise\n",
    "    \"\"\"\n",
    "    # Normalize for comparison\n",
    "    expected_norm = normalize_text(expected_answer)\n",
    "    user_norm = normalize_text(user_answer)\n",
    "    \n",
    "    # Strategy 1: Containment check (best for short answers like \"1896\", \"Au\", \"George Washington\")\n",
    "    # Check if expected answer is contained in user answer\n",
    "    if check_containment(expected_answer, user_answer):\n",
    "        return True\n",
    "    \n",
    "    # Strategy 2: Semantic similarity with question context (for longer, paraphrased answers)\n",
    "    # Include question context in encoding for better understanding\n",
    "    expected_text = f\"{question} {expected_answer}\"\n",
    "    user_text = f\"{question} {user_answer}\"\n",
    "    \n",
    "    # Encode them as vectors\n",
    "    expected_vec = model.encode(expected_text, convert_to_tensor=True)\n",
    "    user_vec = model.encode(user_text, convert_to_tensor=True)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarity = util.pytorch_cos_sim(expected_vec, user_vec).item()\n",
    "    \n",
    "    return similarity > similarity_threshold\n",
    "\n",
    "# Test with the example\n",
    "check_answer(test_case[\"question\"], test_case[\"expected_answer\"], \"Louis XIV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Who was the first President of the United States?------\n",
      "Expected Answer: George Washington \n",
      " LLM Answer: George Washington, who served as the first President from 1789 to 1797 \n",
      "Expected evaluation: True \n",
      " Model evaluation: True\n",
      "------Who was the first President of the United States?------\n",
      "Expected Answer: George Washington \n",
      " LLM Answer: George Washington was the first President of the United States \n",
      "Expected evaluation: True \n",
      " Model evaluation: True\n",
      "------What is the chemical symbol for gold?------\n",
      "Expected Answer: Au \n",
      " LLM Answer: Au \n",
      "Expected evaluation: True \n",
      " Model evaluation: True\n",
      "------What is the chemical symbol for gold?------\n",
      "Expected Answer: Au \n",
      " LLM Answer: The chemical symbol for gold is Au, derived from the Latin word 'aurum' \n",
      "Expected evaluation: True \n",
      " Model evaluation: True\n",
      "------Which is the largest ocean on Earth?------\n",
      "Expected Answer: Pacific Ocean \n",
      " LLM Answer: The Pacific Ocean \n",
      "Expected evaluation: True \n",
      " Model evaluation: True\n",
      "------Which is the largest ocean on Earth?------\n",
      "Expected Answer: Pacific Ocean \n",
      " LLM Answer: The Atlantic Ocean is the largest ocean on Earth \n",
      "Expected evaluation: False \n",
      " Model evaluation: True\n",
      "------Which actor played Tony Stark in the Marvel Cinematic Universe?------\n",
      "Expected Answer: Robert Downey Jr. \n",
      " LLM Answer: Robert Downey Jr. \n",
      "Expected evaluation: True \n",
      " Model evaluation: True\n",
      "------Which actor played Tony Stark in the Marvel Cinematic Universe?------\n",
      "Expected Answer: Robert Downey Jr. \n",
      " LLM Answer: Robert Downey Jr. portrayed Tony Stark/Iron Man in the MCU \n",
      "Expected evaluation: True \n",
      " Model evaluation: True\n",
      "------In which year were the first modern Olympic Games held?------\n",
      "Expected Answer: 1896 \n",
      " LLM Answer: 1896 in Athens, Greece \n",
      "Expected evaluation: True \n",
      " Model evaluation: True\n",
      "------In which year were the first modern Olympic Games held?------\n",
      "Expected Answer: 1896 \n",
      " LLM Answer: The first modern Olympic Games were held in 1896 \n",
      "Expected evaluation: True \n",
      " Model evaluation: True\n"
     ]
    }
   ],
   "source": [
    "for question in questions:\n",
    "    for model_name, llm_answer in question[\"llm_answers\"].items():\n",
    "        correct = check_answer(question[\"question\"], question[\"expected_answer\"], llm_answer[\"answer\"])\n",
    "        print(f\"------{question['question']}------\")\n",
    "        print(\"Expected Answer:\", question['expected_answer'], \"\\n\", \"LLM Answer:\", llm_answer[\"answer\"], \n",
    "        \"\\nExpected evaluation:\",llm_answer[\"is_correct\"],\"\\n\", \"Model evaluation:\", correct)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1,\n",
       " 'category': 'History',\n",
       " 'question': 'Who was the first President of the United States?',\n",
       " 'expected_answer': 'George Washington',\n",
       " 'llm_answers': {'gpt3.5': {'answer': 'George Washington, who served as the first President from 1789 to 1797',\n",
       "   'is_correct': True},\n",
       "  'claude': {'answer': 'George Washington was the first President of the United States',\n",
       "   'is_correct': True}},\n",
       " 'difficulty': 'easy'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_pursuit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
